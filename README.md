
<body class="c15 doc-content"><div><p class="c6"><span class="c13"></span></p></div><p class="c7 title" id="h.nxojjk9m6y2m"><span class="c12">Charity Funding Predictor Report</span></p><h1 class="c10 c17" id="h.g6xehmypjmbs"><span class="c4">Overview</span><span class="c18">&nbsp;</span></h1><p class="c9"><span class="c0">AlphabetSoup is a non-profit organisation, which allocates funding to applicants. The purpose of this project was to build a tool to help in the decision making process, and predict, based on the available features, with upwards of 75% success, whether an applicant for funding would successfully make good use of it. To implement this, I created and trained an artificial neural network (NN) on prior data.<br></span></p><h1 class="c17 c10" id="h.dgqbxiwybu5x"><span class="c4">Results</span><span class="c18">:<br></span></h1><h2 class="c5 c10" id="h.a6cbxcc1g0w1"><span class="c11">Data Preprocessing</span></h2><p class="c5"><span class="c3">The </span><span class="c2">IS_SUCCESSFUL </span><span class="c3">column is the target variable for the model, featuring only binary values of 0 and 1.</span></p><p class="c5"><span class="c0">The feature variables for the model are:</span></p><ul class="c16 lst-kix_g1iozmbsrt02-2 start"><li class="c1 li-bullet-0"><span class="c2">APPLICATION_TYPE</span><span class="c0">&mdash;Alphabet Soup application type</span></li><li class="c1 li-bullet-0"><span class="c2">AFFILIATION</span><span class="c0">&mdash;Affiliated sector of industry</span></li><li class="c1 li-bullet-0"><span class="c2">CLASSIFICATION</span><span class="c0">&mdash;Government organization classification</span></li><li class="c1 li-bullet-0"><span class="c2">USE_CASE</span><span class="c0">&mdash;Use case for funding</span></li><li class="c1 li-bullet-0"><span class="c2">ORGANIZATION</span><span class="c0">&mdash;Organization type</span></li><li class="c1 li-bullet-0"><span class="c2">STATUS</span><span class="c0">&mdash;Active status</span></li><li class="c1 li-bullet-0"><span class="c2">INCOME_AMT</span><span class="c0">&mdash;Income classification</span></li><li class="c1 li-bullet-0"><span class="c2">SPECIAL_CONSIDERATIONS</span><span class="c0">&mdash;Special consideration for application</span></li><li class="c1 li-bullet-0"><span class="c2">ASK_AMT</span><span class="c0">&mdash;Funding amount requested</span></li></ul><p class="c5"><span class="c2">EIN</span><span class="c3">&nbsp;and </span><span class="c2">NAME </span><span class="c3">are identifiers and not relevant for classifying the data; they are neither targets, nor features, and should be removed.</span></p><h2 class="c5 c10" id="h.ww82biq62bj6"><span>Compiling, Training, and Evaluating the Model<br></span></h2><span class="c3">I initially chose two hidden layers, with eight neurons in each layer, and a Relu function in each case. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 517.04px; height: 392.50px;"><img alt="" src="images/image3.png" style="width: 517.04px; height: 392.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h2><h2 class="c5 c10" id="h.6e555qp7kuer"><span class="c3">This is because I wanted to start fairly small, knowing that I could increase it later if necessary, but because the data had a large number of inputs,<br> </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 381.00px; height: 81.00px;"><img alt="" src="images/image1.png" style="width: 381.00px; height: 81.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3"><br>and some numerical columns (e.g. &lsquo;ASK_AMT&rsquo;) &nbsp;had a large number of values,</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 495.50px; height: 211.53px;"><img alt="" src="images/image5.png" style="width: 495.50px; height: 211.53px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c0">&nbsp;<br>I did not want to start too small; i was expecting to have to increase these after a first pass. However, my score on unseen data was only 72.6%, so I did not meet the 75% threshold on this first pass.</span></h2><p class="c5"><span class="c0">I then began to take steps to optimise the data.</span></p><p class="c5"><span class="c3">My first thought was to increase the number of hidden layers by one:</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 505.96px; height: 242.90px;"><img alt="" src="images/image8.png" style="width: 505.96px; height: 242.90px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c0"><br>As that did not improve the accuracy, I increased the number of neurons in each layer:</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 502.00px; height: 234.32px;"><img alt="" src="images/image2.png" style="width: 502.00px; height: 234.32px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c0"><br>As that also did not improve accuracy, I changed the function from relu to tan:</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 507.24px; height: 204.75px;"><img alt="" src="images/image9.png" style="width: 507.24px; height: 204.75px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c0"><br>None of this improved the performance on the model at all, leading me to two more potential improvements: firstly to further process the data, and secondly to allow tensorflow to auto-tune the NN model.</span></p><p class="c5"><span class="c3">I therefore wrote a method to throw out outliers in the data, given defined parameters</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 521.50px; height: 245.10px;"><img alt="" src="images/image11.png" style="width: 521.50px; height: 245.10px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3"><br>and went through each column, choosing a cut-off level for outliers, and removing them.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 484.50px; height: 585.30px;"><img alt="" src="images/image13.png" style="width: 569.04px; height: 585.30px; margin-left: -84.54px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3"><br>This reduced some columns to only one variable, rendering them pointless, so they were dropped.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 510.96px; height: 222.10px;"><img alt="" src="images/image7.png" style="width: 583.78px; height: 222.10px; margin-left: -72.82px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c3">I decided also to try to reduce noise in the data by reducing the two numerical columns into bins (or fewer bins)<br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 284.00px; height: 183.00px;"><img alt="" src="images/image12.png" style="width: 284.00px; height: 183.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 534.50px; height: 176.69px;"><img alt="" src="images/image4.png" style="width: 534.50px; height: 176.69px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c0"><br>All of these simplifications combined reduced the number of input dimension from 49 to 35.</span></p><p class="c5"><span class="c3">I decided to use the TensorFlow library to automatically optimise the hyper parameters on this simplified dataset, rather than continuing to guess at parameters myself.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 539.21px; height: 519.50px;"><img alt="" src="images/image6.png" style="width: 539.21px; height: 519.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3">resulting in a best score of 72.6% - barely any better than any of the other models I had already trained.<br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 398.00px; height: 53.00px;"><img alt="" src="images/image10.png" style="width: 398.00px; height: 53.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c0">Due to this lack of improvement, I had tried increasing the number of neurons per layer from 30 to 100; I saw no further improvement.</span></p><p class="c5"><span class="c0">I decided to see what would happen if I used tensorflow just on the original set of data; it still performed no better.</span></p><p class="c5"><span class="c0">There was one last modification to the data set I tried: keeping in the &lsquo;NAME&rsquo; column rather than removing it at the start (optimise_tensorflow_keep_NAME.ipynb). </span></p><p class="c5"><span class="c3">With this as the only filter different from the default, the NN went on to achieve a satisfying 80% accuracy. However, I find it deeply unethical to include such an identifier as training data, as the model is effectively just learning to stereotype the data based on keywords in the name of the applicant. This reminds me of experiments measuring racial discrimination in the labor market such as &ldquo;Are Emily and Greg More Employable than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination&rdquo; by </span><span class="c3">Marianne Bertrand</span><span class="c3">&nbsp;&amp;</span><span class="c14 c3"><a class="c8" href="https://www.google.com/url?q=https://www.nber.org/people/sendhil_mullainathan&amp;sa=D&amp;source=editors&amp;ust=1659797842838147&amp;usg=AOvVaw3NH5i2WdaAysC2qk4XC4Fa">&nbsp;</a></span><span class="c3">Sendhil Mullainathan </span><span class="c3">(</span><span class="c3 c14"><a class="c8" href="https://www.google.com/url?q=https://www.nber.org/papers/w9873&amp;sa=D&amp;source=editors&amp;ust=1659797842838527&amp;usg=AOvVaw1Dlw-rjGLfFoQXOzWYeZ0i">https://www.nber.org/papers/w9873</a></span><span class="c3">).</span></p><h2 class="c5 c10" id="h.4u7jjb55kocb"><span class="c4">Summary</span><span class="c11">: </span></h2><p class="c5"><span class="c0">Overall, I&rsquo;m happy that I was close to the required value, which is still significantly better than random, but perhaps the data are just too noisy to be perfectly predictable.<br>This is understandable, as the realistic dataset I am working from is a summary of available categories, but these categories are never going to be the whole story - individual circumstances of each applicant, in different parts of the country, with different personal qualities, are also going to have a large effect on whether a grant was used effectively. Also it is worth considering that the data are marked up as a binary of &lsquo;successful&rsquo; or &lsquo;not&rsquo; - when surely in reality this should be a more continuous variable that would suit a regression problem.</span></p><p class="c5"><span class="c0">Due to the data having an identifiable target column, we could try a Supervised Learning model such as scikitlearn on this dataset. These could be Logistic Regression and Random Forest methods. I would imagine Logistic Regression might not cope with the number of variables in the data, whereas Random Forest might work better due to its stochastic approach.</span></p><p class="c6"><span class="c0"></span></p></body>
